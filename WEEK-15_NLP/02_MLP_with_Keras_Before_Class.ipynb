{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-MLP_with_Keras_Before_Class.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "F1A72dTvH684",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Sentiment Analysis with a Multi Layer Perceptron using Keras\n",
        "\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this lab session we will implement a classification model for __sentence classification__ using Keras. Given a sentence our model will predict if it is a positive or negative piece of texts. The dataset we are going to use ranges the polarity annotation from 0 to 5, where 0 denotes extremely negative sentiment,  and 5  is the most  positive. \n",
        "\n",
        "Nevertheless, for this lab we'll  simplify the task, and we will translate the 5-way classification task into 2-way classification task (0 $\\rightarrow$ _negative,_ ;1 $\\rightarrow$ positive),\n",
        "\n",
        "In addition, we will review some of the regularization techniques seen in class. \n",
        "\n",
        "All in all, the main __objectives__ of this first laboratory are the following: \n",
        "- Learn how to build, train and evaluate a Model in Keras\n",
        "- Explore hyperparameters like:\n",
        "  - Optimizers: SGD, ADAGRAD, etc.\n",
        "  - Learning Rates\n",
        "  - Regularization\n",
        "- Plot learning curves for model selection"
      ]
    },
    {
      "metadata": {
        "id": "8KY_HBdDH69E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Loading the data\n",
        "We'll use the same data used in previous session. You need to follow the same steps specified in lab1."
      ]
    },
    {
      "metadata": {
        "id": "7cZ81FP7k39R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Mount Drive files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a-iZBCqtH69B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## for replicability of results\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.set_random_seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EP3e95P1sSmW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sst_home = 'drive/My Drive/kschool-nlp/data/trees/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITmWwxRuH69E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Let's do 2-way positive/negative classification instead of 5-way    \n",
        "def load_sst_data(path,\n",
        "                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(f): \n",
        "            example = {}\n",
        "            example['label'] = easy_label_map[int(line[1])]\n",
        "            if example['label'] is None:\n",
        "                continue\n",
        "            \n",
        "            # Strip out the parse information and the phrase labels---we don't need those here\n",
        "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
        "            example['text'] = text[1:]\n",
        "            data.append(example)\n",
        "    data = pd.DataFrame(data)\n",
        "    return data\n",
        "\n",
        "training_set = load_sst_data(sst_home + 'train.txt')\n",
        "dev_set = load_sst_data(sst_home + 'dev.txt')\n",
        "test_set = load_sst_data(sst_home + 'test.txt')\n",
        "\n",
        "print('Training size: {}'.format(len(training_set)))\n",
        "print('Dev size: {}'.format(len(dev_set)))\n",
        "print('Test size: {}'.format(len(test_set)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iG8N7zucH69I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing and vectorization\n",
        "\n",
        "Once data is loaded the next step is to preprocess it to obtain the vectorized form (i.e. the process of transforming text into numeric tensors), which basically consist of:\n",
        "\n",
        "- Tokenization, tipically segment the text into words. (Alternatively, we could segment text into characters, or extract n-grams of words or characters.)\n",
        "- Definition of the dictionary index and vocabulary size (in this case we set to 1000 most frequent words)\n",
        "- Transform each sentence into a vector. \n",
        "\n",
        "In this lab, we will follow the Bag of Words approach. "
      ]
    },
    {
      "metadata": {
        "id": "ySSvGw_bH69J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Shuffle dataset\n",
        "training_set = shuffle(training_set)\n",
        "dev_set = shuffle(dev_set)\n",
        "test_set = shuffle(test_set)\n",
        "\n",
        "# Obtain text and label vectors, and tokenize the text\n",
        "train_texts = training_set.text\n",
        "train_labels = training_set.label\n",
        "\n",
        "dev_texts = dev_set.text\n",
        "dev_labels = dev_set.label\n",
        "\n",
        "test_texts = test_set.text\n",
        "test_labels = test_set.label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1NXPO1Tcs5L_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import text\n",
        "\n",
        "# Create a tokenize that takes the 1000 most common words\n",
        "tokenizer = text.Tokenizer(num_words=1000)\n",
        "\n",
        "# Build the word index (dictionary)\n",
        "tokenizer.fit_on_texts(train_texts) # Create word index using only training part\n",
        "\n",
        "# Vectorize texts into one-hot encoding representations\n",
        "x_train = tokenizer.texts_to_matrix(train_texts, mode='binary')\n",
        "x_dev = tokenizer.texts_to_matrix(dev_texts, mode='binary')\n",
        "x_test = tokenizer.texts_to_matrix(test_texts, mode='binary')\n",
        "          \n",
        "y_train = train_labels\n",
        "y_dev = dev_labels\n",
        "y_test = test_labels\n",
        "\n",
        "print('Text of the first examples: \\n{}\\n'.format(train_texts[0]))\n",
        "print('Vector of the first example:\\n{}\\n'.format(x_train[0]))\n",
        "print('Binary representation of the output:\\n{}\\n'.format(y_train[0]))\n",
        "\n",
        "\n",
        "print('Shape of the training set (nb_examples, vector_size): {}'.format(x_train.shape))\n",
        "print('Shape of the validation set (nb_examples, vector_size): {}'.format(x_dev.shape))\n",
        "print('Shape of the test set (nb_examples, vector_size): {}'.format(x_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGcJPyCWtvpv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Recorver the word index that was created with the tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "print('Found {} unique tokens.\\n'.format(len(word_index)))\n",
        "\n",
        "word_count = tokenizer.word_counts\n",
        "print(\"Show the most frequent word index:\")\n",
        "\n",
        "for i, word in enumerate(sorted(word_count, key=word_count.get, reverse=True)):\n",
        "    print('   {} ({}) --> {}'.format(word, word_count[word], word_index[word]))\n",
        "    if i == 9: \n",
        "        print('')\n",
        "        break\n",
        "\n",
        "for i, word in enumerate(sorted(word_count, key=word_count.get, reverse=False)):\n",
        "    print('   {} ({}) --> {}'.format(word, word_count[word], word_index[word]))\n",
        "    if i == 9: \n",
        "        print('')\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvF5nZpIt4lH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check what we obtain when we vectorize words that are out of the index (out of vocabulary words)."
      ]
    },
    {
      "metadata": {
        "id": "sU8uwVppt6kC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "oov_sample = ['saddam', 'plausible'] \n",
        "sequences = tokenizer.texts_to_matrix(oov_sample)\n",
        "print(sum(sequences[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VurlsAWNuA0Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is possible to obtain the lists of integers indices instead of the one-hot binary representation."
      ]
    },
    {
      "metadata": {
        "id": "BteW3HAkuC7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_index_inverse = {word_index[k]:k for k in word_index}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SYaUyelvuDFs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Turns strings into list of integer indices\n",
        "one_hot_results = tokenizer.texts_to_sequences(train_texts)\n",
        "print(one_hot_results[0])\n",
        "print(train_texts.iloc[0])\n",
        "print([word_index_inverse[x] for x in one_hot_results[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTtrQlPtuDCI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnL43-EHH69L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Building the model\n",
        "\n",
        "When we build a neural network we usually take into account the following points:\n",
        "- The __layers__, and how they are combined (that is, the structure and parameters of the model)\n",
        "- The __input__ and the __labeled output__ data that the model needs to map.\n",
        "- __Loss function__ that signals how well the model is doing.\n",
        "- The __optimizier__ which defines the learning procedure.\n",
        "\n",
        "In this very first session we'll keep all this very simple. Keras provide a simple framework for combining layers. There are available two types of classes for building the model: The _Sequential_ Class and the _functional_ API. The later is dedicated to DAGs structures, which let you to build arbitrary models. The former is for linear stacks of layers, which is the most common and simplest archicture. \n",
        "\n",
        "In this session, we will build a __Multi Layer Perceptron__ with one hidden units, which is one of the most simple neural network model. More complicated models, like LSTM and CNN, will be learnt in the next lab sessions.\n",
        "\n",
        "For that we'll make use of a fully connected (```Dense```) layers with a ```relu``` activation. In this case the hidden layer will have 16 hidden units (feel free to explore different dimensionality of hidden units).\n",
        "\n",
        "Remenber that applying ```Dense``` layer with ```relu``` activation we are implementing the following tensor operation:\n",
        "\n",
        "```\n",
        "output = relu(dot(W, input) + b\n",
        "```\n",
        "\n",
        "where `relu` is the element-wise activation function, ```W``` is a weights matrix created by the layer, and ```b``` is a bias vector created by the layer.\n",
        "\n",
        "Remenber from the slides that mathematically can written as follows:\n",
        "> $sigmoid(W^{T}X + b)$\n",
        "\n",
        "\n",
        "Regarding input data, we will use the __one-hot encoding__. We'll set ```(binary) cross-entropy``` as a __loss function__ and ```rmsprop```, a variant of the _Stochastic Gradient Descent_, as the __optimizer__.\n",
        "\n",
        "Feel free to explore different loss-functions (e.g. MSE) and optimizers (e.g. ADAM) you can improve the model (see Exercise 2, below).\n",
        "\n",
        "### Exercise 1\n",
        "Answer the following questions:\n",
        "- What does having 16 hidden units mean? What the size of matrix ```W```?\n",
        "\n",
        "-----\n",
        "\n",
        "Increasing the number of hidden units we are allowing the network to learn more complex representations, but at the same time we are making the network more computationally expensive and may lead to overfit the training data.\n",
        "\n",
        "Regarding the architecture, there are three main decisions that we need to take:\n",
        "- The number of layers\n",
        "- The number of hidden units for each layers\n",
        "- Activation function of the layers"
      ]
    },
    {
      "metadata": {
        "id": "u4OKtGMRH69M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below implements a fully connected archicteture with only one intermediate layer and an output layer that predicts the sentiment of the input review. \n",
        "\n",
        ">>>>>>>![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/Two_layers_NN.png)"
      ]
    },
    {
      "metadata": {
        "id": "yUtayUEPH69M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "input_size = x_train[0].shape[0] ## vector length equals to vocabulary size.\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=16, activation='relu', input_shape=(input_size,)))\n",
        "model.add(Dense(units=1, activation='sigmoid'))  \n",
        "# Note that we do not need to indicate the input shape for the sucessives layes\n",
        "\n",
        "# Compile the model using a loss function and an optimizer.\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "krauvCcEH69P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_dev, y_dev), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rpvN8CYMH69S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgxZ091cx_AG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_curve(history_, metric_, title_, legend_):\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  plt.plot(history_.history[metric_])\n",
        "  plt.plot(history_.history['val_'+metric_])\n",
        "  plt.title(title_)\n",
        "  plt.ylabel(metric_)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(legend_, loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_curve(history, 'acc', 'Accuracy', ['train', 'test'])\n",
        "plot_curve(history, 'loss', 'Loss', ['train', 'test'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qcXd9K-0v59T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(mini-) **Exercise**: Train the model with different optimizers: SGD, ADA, ADAGRAD, etc"
      ]
    },
    {
      "metadata": {
        "id": "KBi2Ioeyv9KE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to-do"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnlvfQVrwQwl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Evaluating the model\n",
        "\n",
        "Once we fit the model we can use the method ```model.evaluate()``` to obtain the accuracy on test set."
      ]
    },
    {
      "metadata": {
        "id": "BT0-7B6dwVum",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Accuracy: \", score[1])\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7dx_Zes0H69W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 (home)\n",
        "\n",
        "Plots show that model ends up overfitting the training data. One way to prevent overfitting is to stop training once accuracy in the validation set starts decreasing. \n",
        "- Could you retrain the model from the scratch for only four epochs? \n",
        "\n",
        "- Optionally, Keras provides early stopping mechanism as callback object(https://keras.io/callbacks/#earlystopping) that could be used when fiting the model:\n",
        "\n",
        "```\n",
        "from keras.callbacks import EarlyStopping\n",
        "...\n",
        "early_stop = EarlyStopping(monitor='acc', patience=1)\n",
        "...\n",
        "history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_dev, y_dev), verbose=1, callbacks=[early_stop])\n",
        "\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "QNOwtzryH69Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "- Optionally you can try different activations (e.g ```tahn```, ```sigmoid```) instead of ```relu```.\n",
        "- Or try different loss function like mean_squared_error (```mse```).\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "2FJO8o9nwgUc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Model Tuning\n",
        "\n",
        "### 5.1. Effect of Learning Rate \n",
        "\n",
        "The model in Section 3 uses default values of learning rate, and does not use any type of regularization.\n",
        "\n",
        "You can check Keras API to learn how to use and set up different optimizers: \n",
        "- https://keras.io/optimizers/\n",
        "- https://keras.io/regularizers/\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nQNAZnpHxm1I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 3\n",
        "\n",
        "In this exercise we'll focus in the importance of the learning rate. We'll compare a large and a small learning rate with the default one. \n",
        "\n",
        "\n",
        "__Run the following cells and answers to the next question:__\n",
        "\n",
        "- Why we obtain such a different plots with each learning rate?\n",
        "\n",
        "- What is the difference when comparing the following curves:\n",
        "   - ```train large```_vs_ ```train orig```\n",
        "   - ```dev large```_vs_ ```dev orig```\n",
        "\n",
        "- And the following ones:\n",
        "   - ```train small```_vs_ ```train orig```\n",
        "   - ```dev small```_vs_ ```dev orig```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DlPz4kFrwfYZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Example of using optimizer object\n",
        "\n",
        "from keras import optimizers, regularizers\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "# add L2 weight regularization to logistic regression\n",
        "regularizer = regularizers.l2(0.)\n",
        "model2.add(Dense(units=1, activation='sigmoid', input_shape=(input_size,), kernel_regularizer=regularizer))\n",
        "\n",
        "# Init rmsprop\n",
        "rmsprop_small = optimizers.RMSprop(lr=0.000001)\n",
        "rmsprop_large = optimizers.RMSprop(lr=0.5)\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer=rmsprop_small, metrics=['accuracy'])\n",
        "history_small_lr= model2.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer=rmsprop_large, metrics=['accuracy'])\n",
        "history_large_lr= model2.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e7GtezG8ylHv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history_large_lr.history['acc'])\n",
        "plt.plot(history_large_lr.history['val_acc'], linestyle='--')\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'], linestyle='--')\n",
        "\n",
        "plt.plot(history_small_lr.history['acc'])\n",
        "plt.plot(history_small_lr.history['val_acc'], linestyle='--')\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train large', 'dev large', 'train orig', 'dev orig', 'train small', 'dev small'], loc='center right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QmOBb9L2y8_g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.2. Effect of regularization (more on next section)\n",
        "\n",
        "Regularization is a method to avoid overfitting. It add a penalization term on the weights, so they are kept small.\n"
      ]
    },
    {
      "metadata": {
        "id": "V1IkXQYMzx5d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Exercise 4\n",
        "\n",
        "In this session we'll focus in the effect of regularization. We'll compare regularized model agains non-regularized one.\n",
        "\n",
        "__Please run the following cell and answers to the next question__:\n",
        "\n",
        "What is the effect of including a regularization term? Is it always a good thing to be included?\n",
        "\n",
        "\n",
        "------\n",
        "\n",
        "The plots might not be the expected, but you should note that we reduced the vocabulary to only 1000 most frequent words in training. Anyway, you should see the differences of learning curves when training with and without regularization.\n",
        "\n",
        "----"
      ]
    },
    {
      "metadata": {
        "id": "rkbN-GpQHov7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras import optimizers, regularizers\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "# add L2 weight regularization to logistic regression\n",
        "regularizer = regularizers.l2(0.0001)\n",
        "model2.add(Dense(units=1, activation='sigmoid', input_shape=(input_size,), kernel_regularizer=regularizer))\n",
        "\n",
        "# Init rmsprop\n",
        "rmsprop = optimizers.RMSprop() \n",
        "model2.compile(loss='binary_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
        "history_reg = model2.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VyBw-phwNhaA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'], linestyle='--')\n",
        "\n",
        "plt.plot(history_reg.history['acc'])\n",
        "plt.plot(history_reg.history['val_acc'], linestyle='--')\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train orig', 'dev orig', 'train reg', 'dev reg'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y3yjo6FqH69a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. Adding new layers\n",
        "In this section we will extend the model by adding new fully connected layer. By adding new layers we are increasing the capacity of the model we are not reducing the overfitting, but for the moment  we do not care about this. \n",
        "\n",
        "### Exercise 5\n",
        "- The code below defines a model with two hidden layers,  add a third intermediate layer with 16 hidden units and ```relu``` activation."
      ]
    },
    {
      "metadata": {
        "id": "CXgr-wuJH69a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "input_size = x_train[0].shape[0] ## vector length equals to vocabulary size.\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=16, activation='relu', input_shape=(input_size,)))\n",
        "model.add(Dense(units=16, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "# Note that we do not need to indicate the input shape for the sucessives layers\n",
        "\n",
        "# Compile the model using a loss function and an optimizer.\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_dev, y_dev), verbose=1)\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Z10NGbeH69f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7. Regularization techniques\n",
        "\n",
        "We will apply the following ones:\n",
        "- Reducing the network size\n",
        "- Adding weight regularization\n",
        "- Adding dropout\n",
        "\n",
        "### 7.1 Reducing the network size\n",
        "One way to prevent overfitting is to reduce the size of the model. As we know the size of the model is measured with the number of parameters that we need to learn. Remenber that the number of parameters are determined by the number of layers and the number of units per layer.\n",
        "\n",
        "### Exercise 6\n",
        "Run the following cell of code and try to answer the following question:\n",
        "- Can you describe the relation of training and validation loss curves when traning with less parameters?"
      ]
    },
    {
      "metadata": {
        "id": "zZ5qvOq3H69g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "input_size = x_train[0].shape[0] ## vector length equals to vocabulary size.\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=4, activation='relu', input_shape=(input_size,)))\n",
        "model.add(Dense(units=4, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "# Note that we do not need to indicate the input shape for the sucessives layes\n",
        "\n",
        "# Compile the model using a loss function and an optimizer.\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)\n",
        "\n",
        "\n",
        "# Define and train the \"original\" model of two hidden layers of 16 output units each\n",
        "model = Sequential()\n",
        "model.add(Dense(units=16, activation='relu', input_shape=(input_size,)))\n",
        "model.add(Dense(units=16, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "history_orig = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_dev, y_dev), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FEtyKXS41EPB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history_orig.history['loss'])\n",
        "plt.plot(history.history['loss'], linestyle='--')\n",
        "\n",
        "plt.plot(history_orig.history['val_loss'])\n",
        "plt.plot(history.history['val_loss'], linestyle='--')\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train: orig model', 'train: small model', 'dev: orig model', 'dev: small model'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfYNUCTpH69j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can compare the loss of bigger model with a higher _capacity_. Note that a model with a higher number of parameters has more memorization capacity and consequently can show a poorer generalization with higher risk of overfitting to training data.\n",
        "\n",
        "### Exercise 7\n",
        "Run the following cell of code and try to answer the following question:\n",
        "- Can you describe the relation of training and validation loss curves when traning with more parameters?"
      ]
    },
    {
      "metadata": {
        "id": "vNqerudcH69j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "input_size = x_train[0].shape[0] ## vector length equals to vocabulary size.\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=512, activation='relu', input_shape=(input_size,)))\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "# Note that we do not need to indicate the input shape for the sucessives layes\n",
        "\n",
        "# Compile the model using a loss function and an optimizer.\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1r54vDaZ1tr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history_orig.history['loss'])\n",
        "plt.plot(history.history['loss'], linestyle='--')\n",
        "\n",
        "plt.plot(history_orig.history['val_loss'])\n",
        "plt.plot(history.history['val_loss'], linestyle='--')\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train: orig model', 'train: bigger model', 'dev: orig model', 'dev: bigger model'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihL1w3lz193v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history_orig.history['loss'])\n",
        "plt.plot(history_orig.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train: orig model', 'dev: orig model'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2osD1J92H69n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### 7.2 Adding weight regularization\n",
        " \n",
        "Another common way to try avoiding overfitting is to put constraints on the complexity of a network by forcing its weights to only take small values, which makes the distribution of weight values more \"regular\". This is called _weight regularization_, and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n",
        "\n",
        "- __L1 regularization__, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n",
        "\n",
        "- __L2 regularization__, where the cost added is proportional to the square of the value of the weights coefficients.\n",
        "\n",
        "In Keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. In this case, we'll use the L2 norm to regularize the weights of the model."
      ]
    },
    {
      "metadata": {
        "id": "kZlKJvyJH69o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import regularizers\n",
        "\n",
        "input_size = x_train[0].shape[0] ## vector length equals to vocabulary size.\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu', input_shape=(input_size,)))\n",
        "model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                          activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model using a loss function and an optimizer.\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history_l2 = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SC2EvGph3K8i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history_orig.history['loss'])\n",
        "plt.plot(history_l2.history['loss'], linestyle='--')\n",
        "\n",
        "plt.plot(history_orig.history['val_loss'])\n",
        "plt.plot(history_l2.history['val_loss'], linestyle='--')\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train: orig', 'train: l2', 'dev: orig', 'dev: l2'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsKGvsHtH69u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 8\n",
        "\n",
        "- Explore different regularization weigths (e.g. 0.001, 0.01, 0.1). Do you see any difference in the learning curves?\n",
        "- You can try __L1 regularization__, or both together. \n",
        "\n",
        "```\n",
        "# L1 regularization\n",
        "regularizers.l1(0.001)\n",
        "\n",
        "# L1 and L2 regularization at the same time\n",
        "regularizers.l1_l2(l1=0.001, l2=0.001)\n",
        "```\n",
        "\n",
        "-----"
      ]
    },
    {
      "metadata": {
        "id": "hdGq32qwH69v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7.3 Adding dropout\n",
        "Another popular regularization technique for deep learning is _dropout_. It has be proven to very successful in many cases, which state-of-the-art of the architecture can be improved around 1-2% of accuracy. \n",
        "\n",
        "The algorithm is simple: At every training step every unit has a probability $p$ of being dropped out (it will not take into account during the training step, setting it to zero).  In Keras user needs to set a _dropout rate_, which is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time."
      ]
    },
    {
      "metadata": {
        "id": "WkvxGDE5H69x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "input_size = x_train[0].shape[0] ## vector length equals to vocabulary size.\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=16, activation='relu', input_shape=(input_size,)))\n",
        "# we add a drop-out layer after the first fully connected layer\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(units=16, activation='relu'))\n",
        "# we add a drop-out layer after the second fully connected layer\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model using a loss function and an optimizer.\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_dev, y_dev), verbose=0)\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history_orig.history['loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "\n",
        "plt.plot(history_orig.history['val_loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train: orig model', 'train: dropout', 'dev: orig model', 'dev: dropout'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GerXL7l9H69z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 8\n",
        "- Try different dropout rates and decide which one is the best."
      ]
    }
  ]
}