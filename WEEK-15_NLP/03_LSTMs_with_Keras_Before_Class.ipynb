{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-LSTMs_with_Keras_Before_Class.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "imyaVBbcIBVq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 3: Sentiment Analysis with LSTMs using Keras\n",
        "\n",
        "In this lab session, you'll implement an **RNN-based sentence classifier**. Plain old RNNs aren't very good at sentiment classification, so we will use Keras to implement a more advanced __Long Short Term Memory__ (LSTM) based sentiment classifier. \n",
        "\n",
        "\n",
        "__Objectives__: In this lab session will learn the following:\n",
        "\n",
        "- Word embedding representation\n",
        "- Preprocessing data for recurrent archictures (sequence padding)\n",
        "- Implementation of LSTM-based classifier\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "Remember from the theory that Recurrent Neural Networks apply over and over the same function (the recursive cell) to every token in the sequence. In a simplified version the next token is combined with the output of the previous _state_ (contains the information of what has been seen so far) into the recursive function, so that the whole sequence is represented in a single vector. \n",
        "\n",
        "Figure below shows an unrolled LSTM archicture, in which input text sequence, once tokenized and obtained the word index, is represented by word embeddings. Emebdding lookup layer takes a list of word indexes and returns a list word embeddings (low-dimensional dense vectors that represent words). These word embeddings are what actually fed to sentence encoder. Finally, the last output of the LSTM is fed to a fully connected Dense layer. As we'll learn, this is fairly easy to code with Keras.\n",
        "\n",
        "\n",
        "![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/LSTM_sentiment.png)\n",
        "\n",
        "\n",
        "Advantages of these types of architectures:\n",
        "\n",
        "- We do not need to show the whole sequence to the model. Actually, each input token is processed independently and current state is kept in memory for the next step.\n",
        "- We save memory as we share the weights for each time-step."
      ]
    },
    {
      "metadata": {
        "id": "yWHDCCqPIBVr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Loading the data\n",
        "We'll use the same data used in previous session."
      ]
    },
    {
      "metadata": {
        "id": "ihLkOMT9OJDt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Mount Drive files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKrb7ihDrfmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sst_home = 'drive/My Drive/kschool-nlp/data/trees/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZhrDTCkIBVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "## for replicability of results\n",
        "np.random.seed(1)\n",
        "tf.set_random_seed(2)\n",
        "\n",
        "# Let's do 2-way positive/negative classification instead of 5-way    \n",
        "def load_sst_data(path,\n",
        "                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(f): \n",
        "            example = {}\n",
        "            example['label'] = easy_label_map[int(line[1])]\n",
        "            if example['label'] is None:\n",
        "                continue\n",
        "            \n",
        "            # Strip out the parse information and the phrase labels---we don't need those here\n",
        "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
        "            example['text'] = text[1:]\n",
        "            data.append(example)\n",
        "    data = pd.DataFrame(data)\n",
        "    return data\n",
        "\n",
        "def pretty_print(example):\n",
        "    print('Label: {}\\nText: {}'.format(example['label'], example['text']))\n",
        "\n",
        "training_set = load_sst_data(sst_home+'/train.txt')\n",
        "dev_set = load_sst_data(sst_home+'/dev.txt')\n",
        "test_set = load_sst_data(sst_home+'/test.txt')\n",
        "\n",
        "# Shuffle dataset\n",
        "training_set = shuffle(training_set)\n",
        "dev_set = shuffle(dev_set)\n",
        "test_set = shuffle(test_set)\n",
        "\n",
        "# Obtain text and label vectors\n",
        "train_texts = training_set.text\n",
        "train_labels = training_set.label\n",
        "\n",
        "dev_texts = dev_set.text\n",
        "dev_labels = dev_set.label\n",
        "\n",
        "test_texts = test_set.text\n",
        "test_labels = test_set.label\n",
        "\n",
        "\n",
        "print('Training size: {}'.format(len(training_set)))\n",
        "print('Dev size: {}'.format(len(dev_set)))\n",
        "print('Test size: {}'.format(len(test_set)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9iodJaVRIBVx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing: Tokenization, Sequence Padding\n",
        "\n",
        "\n",
        "**Word representation**\n",
        "\n",
        "Once data is loaded the next step is to preprocess it to obtain the vectorized form (i.e. the process of transforming text into numeric tensors), which basically consist of:\n",
        "\n",
        "- Tokenization, tipically segment the text into words. (Alternatively, we could segment text into characters, or extract n-grams of words or characters.)\n",
        "- Definition of the dictionary index and vocabulary size (in this case we set to 1000 most frequent words)\n",
        "- Transform each **word** into a vector. \n",
        "\n",
        "\n",
        "There are multiple ways to vectorize tokens. The main two are the following: ___One-hot encoding___ and ___word embedding___. In this lab, we'll use first use Keras basic tools to obtain the one-hot encoding, and we'll leave word embeddings for the next section.\n",
        "\n",
        "### 2.1. One-hot encoding of the data\n",
        "\n",
        "One-hot encoding is the most basic way to convert a token into a vectort. Here, we'll turn the input vectors into (0,1)-vectors. The process consist of associating a unique integer-index with every word in the vocabulary.\n",
        "\n",
        ">>>>>![](http://ixa2.si.ehu.es/~jibloleo/uc3m_dl4nlp/img/vectorize_small.png)\n",
        "\n",
        "\n",
        "For example, if the tokenized vector contains a word that its dictionary index is 14, then in the processed vector, the 14th entry of the vector will be 1 and the rest will set to 0.\n",
        "\n",
        "Note that when using keras built-in tools for indexing, ```0``` is a reserved index that won't be assigned to any word.\n",
        "\n",
        "\n",
        "**Sentence representation**\n",
        "\n",
        "When process data to feed a recurrent archicture, we need to do it differently compared to what we have seen so far. Unrolling each sequence one by one would take for ever, as we would lost the hability for parallelization. In deep learning framework learning is done by mini-batching the training data, which requires having same sequence length for all the input examples in the mini-batch. \n",
        "\n",
        "In order to do mini-batch (there are more sophisticated alternatives):\n",
        "- Choose a single unrolling constant N (e.g. max sequence length)\n",
        "- Pad first words with zeros (shifting right)\n",
        "\n",
        "\n",
        "There are more sophisticated alternivative like shuffling examples by sentence length (set N to max. length in mini-batch). \n",
        "\n",
        "In the following chunk of code, we will use Keras built-in functions for tokenization and padding sequence.\n"
      ]
    },
    {
      "metadata": {
        "id": "n5GlBviXIBVy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import preprocessing\n",
        "\n",
        "max_words = 10000\n",
        "max_seq = 40\n",
        "\n",
        "# Create a tokenize that takes the 10000 most common words\n",
        "tokenizer = preprocessing.text.Tokenizer(num_words=max_words)\n",
        "\n",
        "# Build the word index (dictionary)\n",
        "tokenizer.fit_on_texts(train_texts) # Create word index using only training part\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# Get data as a lists of integers\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "dev_sequences = tokenizer.texts_to_sequences(dev_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding data: Turn the lists of integers into a 2D integer tensor of shape `(samples, max_seq)`\n",
        "x_train = preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_seq)\n",
        "x_dev = preprocessing.sequence.pad_sequences(dev_sequences, maxlen=max_seq)\n",
        "x_test = preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_seq)\n",
        "\n",
        "y_train = train_labels\n",
        "y_dev = dev_labels\n",
        "y_test = test_labels\n",
        "\n",
        "print('Shape of the training set (nb_examples, vector_size): {}'.format(x_train.shape))\n",
        "print('Shape of the validation set (nb_examples, vector_size): {}'.format(x_dev.shape))\n",
        "print('Shape of the test set (nb_examples, vector_size): {}'.format(x_test.shape))\n",
        "\n",
        "# Print some examples\n",
        "print()\n",
        "print('TEXT: {}\\nPADDED: {}'.format(train_texts.iloc[0], x_train[0]))\n",
        "print()\n",
        "print('TEXT: {}\\nPADDED: {}'.format(train_texts.iloc[1], x_train[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "26bKe9rhIBV1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. First LSTM-based classifier\n",
        "\n",
        "In this section we will build our first LSTM-based classifier in Keras. Note that the LSTM layer, like the rest of layers in Keras, takes inputs of shape ```[batch_size, sequence_length, input_features]```, and it is the reason of why we perform padding when preprocessed the data in the previous section.\n",
        "\n",
        "Note that after the padding our data still is 2D tenfor of shape ```[batch_size, sequence_length]```. Transformation from 2D to 3D is done with```Embedding``` layer in Keras, which takes the 2D tensor (```[batch_size, sequence_length]```). ```sequence_length``` is an entry of a list of word indexes, where all the entries in the bacth have the same length. (That's why we padded with zeros the shorter sequences).\n",
        "\n",
        "```Embedding``` layer return a tensor of shape ```[batch_size, sequence_length, embedding_size]```. This can be understood like adding the corresponding embedded vector to each word in the sequence. The layer can be initialized at radom and learn with backpropagation, or use precomputed embedding vector like _Word2vec_ or _Glove_.\n",
        "\n",
        "Once our data is represented with 3D tensors we can use directly the ```LSTM``` layer. For this task we will combine three Keras layers in this specific order: \n",
        "\n",
        "- ```Embedding``` layer: it will transfor the data from 2D to 3D by addig associated embeddings to words in the sequences. In the constructor we need to specify two arguments: \n",
        "   - input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
        "   - output_dim: int >= 0. Dimension of the dense embedding.\n",
        "   \n",
        "- ```LSTM``` layer: It will encode the input sequnces and return output tensor. For the LSTM we need to specify the number of units of the LSTM:\n",
        "   - units: Positive integer, dimensionality of the output space.\n",
        "   \n",
        "- ```Dense```: It will take the output of the LSTM as input and perform the classification. "
      ]
    },
    {
      "metadata": {
        "id": "YDPB0OnfIBV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "max_words = 10000\n",
        "embedding_size = 128\n",
        "lstm_hidden_size = 128 #128\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Define and add Embedding layer to the model. \n",
        "#    Note we are using mask_zero=True as we want to ignore the '0' words in the padding\n",
        "model.add(Embedding(max_words, embedding_size, mask_zero=True))\n",
        "# After the Embedding layer, \n",
        "# our activations have shape `(batch_size, max_seq, embedding_size)`.\n",
        "\n",
        "# 2. Define and add LSTM layer to the model.\n",
        "model.add(LSTM(lstm_hidden_size))\n",
        "\n",
        "# 3. Define and add Dense layer to the model\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model using a loss function and an optimizer.\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nN_eoM7QIBV4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_dev, y_dev), verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WUJiIghOIBV6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "erwEw_u4IBV-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        "\n",
        "- Try different embedding sizes and number of LSTM units. Do you see any differences in loss and accuracy curves? What happens very small embedding size (e.g ```embedding_size = 8```) or LSTM units (e.g ```units = 16```)? And what happens when we do the opposite thing?\n",
        "\n",
        "- __Hint:__ Plotting original model's loss curve and your choice's curve will help you analysing the differences.\n",
        "\n",
        "- __Hint__: Experiments take longer than in the previous labs (model complexity has increase, as well as the vocabulary size), you can increase ```batch_size``` in order to speed up the experiments."
      ]
    },
    {
      "metadata": {
        "id": "GelYNPBGIBV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "\n",
        "- It is sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. How would you do it? \n",
        "\n",
        "- __Hint:__ You have to get all intermediate layers to return full sequences. Recurrent layers in Keras run in two modes that return different type of tensors:\n",
        "  - The first one returns the last output of each input sequence $\\rightarrow$ ```[batch_size, output_features]```\n",
        "  - The second one returns the full sequence of successive output for each time-step $\\rightarrow$ ```[batch_size, sequence_length, output_features]```\n",
        "  \n",
        "- This two modes are controlled by ```return_sequences``` argument in the layer constructor."
      ]
    },
    {
      "metadata": {
        "id": "F6utxoqCIBV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Second LSTM: Initialized with Glove\n",
        "\n",
        "When the training set is not large enough usually it is a good idea to use precomputed word embeddings as we can add some external knowledge that it is very difficult to acquire from the training set."
      ]
    },
    {
      "metadata": {
        "id": "5RDe1G3WIBWA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.1 Reading precomputed embeddings\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EU4Jdz14IBWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read(file, threshold=0, dim=50, word_index=None):\n",
        "    max_words = 400000 if threshold <= 0 else min(threshold, 400000)\n",
        "    \n",
        "    embeddings = {}\n",
        "    lines = file.read()\n",
        "    file.close()\n",
        "    lines = lines.decode('utf8')\n",
        "    for line in lines.split('\\n'):\n",
        "        vec = line.split(' ')\n",
        "        word = vec[0]\n",
        "        coefs = np.asarray(vec[1:], dtype='float32')\n",
        "        embeddings[word] = coefs\n",
        "    \n",
        "    matrix = np.zeros((max_words, dim))\n",
        "    for word, i in word_index.items():\n",
        "      embedding_vector = embeddings.get(word)\n",
        "      if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "          # Words not found in embedding index will be all-zeros.\n",
        "          matrix[i] = embedding_vector\n",
        "    return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "adFfdBbfIBWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import bz2\n",
        "\n",
        "# Read input embeddings\n",
        "glove_home = 'drive/My Drive/kschool-nlp/data/word-embeddings/'\n",
        "embsfile = bz2.open(glove_home + 'glove.6B.50d.txt.bz2')\n",
        "embedding_matrix = read(embsfile, threshold=max_words, word_index=word_index)\n",
        "\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9q4XdGYibm0Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_matrix[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etXlc5ZAbYEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p10B3vTzIBWF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.2 Build LSTM based model\n",
        "\n",
        "We will be using the same architecture as before. Only difference is that now we are going to use an ```embedding_size``` of 50, and the model will be compiled later (after we load the Glove embeddings)."
      ]
    },
    {
      "metadata": {
        "id": "yq4PplCsIBWG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "max_words = 10000\n",
        "embedding_size = 50\n",
        "lstm_hidden_size = 128\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Define and add Embedding layer to the model\n",
        "model.add(Embedding(max_words, embedding_size, mask_zero=True))\n",
        "# After the Embedding layer, \n",
        "# our activations have shape `(batch_size, max_seq, embedding_size)`.\n",
        "\n",
        "# 2. Define and add LSTM layer to the model.\n",
        "model.add(LSTM(lstm_hidden_size))\n",
        "\n",
        "# 3. Define and add Dense layer to the model\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4XgU_JQ3IBWI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.3 Load precomputed weights\n",
        "\n",
        "Once we created the embedding matrix (in Section 4.1) in the correct format, we can easily load it into the Embedding layer. Remember that the matrix is of shape ```[max_words, embedding_size]```, where each entry ```i``` contains the embedding vector of the word of index ```i```, created with the tokenizer. Note that the index 0 is not supposed to stand for any word or token.\n",
        "\n",
        "Embedding layer is the first layer in our model and we can access it with ```model.layers``` list. Once we get it we can initialize the weight as in the code below. \n",
        "\n",
        "In addition we can freeze the weights so that we avoid updating during training and avoid forgetting what they already know. This is done by setting the attribute ```trainable``` of the layer to ```False```."
      ]
    },
    {
      "metadata": {
        "id": "QdH5qCN2IBWK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.layers[0].set_weights([embedding_matrix]) # These are the key step!!!\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PkmI2CjqIBWP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.4. Train and evaluate\n",
        "\n",
        "Training and evaluation is done as in previous models. First we need to compile it (loss, optimizer and evaluation metric are indicated), then fit the model with the training data."
      ]
    },
    {
      "metadata": {
        "id": "aGwwrnGBIBWQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_dev, y_dev), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Np5HNKvYIBWS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p97d8pbf2LKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history10 = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_dev, y_dev), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4E2VTo022e-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['loss'] + history10.history['loss'])\n",
        "plt.plot(history.history['val_loss'] + history10.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'] + history10.history['acc'])\n",
        "plt.plot(history.history['val_acc'] + history10.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#score = model.evaluate(x_test, y_test, verbose=1)\n",
        "#print(\"Accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fuuHpCZuIBWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model does not show any improvements compared to models of the previous section. The reason for that can be  many, for example:\n",
        "  - Vocabulary size is not big enough so we can expect many unknown word when encoding the sentence (not checked).\n",
        "  - Maximum sequence length might be too small so we are leaving out some important information (not checked).\n",
        "  - Training size is not too large, which in those cases simpler model perform usually perform better (due to the overfitting).\n",
        " \n",
        " \n",
        "The loss plot show huge overfitting of the model: Training loss decreases very fast while development loss increases over time. \n",
        "\n",
        "### Exercise 3\n",
        "\n",
        "- In previous lab session we learn different techniques to avoid overfitting the model. In this case ```Dropout``` seems promising. Try adding a drop-out layer in the LSTM-based model. \n",
        "\n",
        "\n",
        "### Exercise 4\n",
        "- Another technique to fight overfitting is to reduce model size. Gated Recurrent Units (```GRU```) are a simpler version of the LSTMs. They have a smaller number of units. Try learning a new model based on GRU layers. \n",
        "\n",
        "- __Hint__: Check the API for the GRU layer $\\rightarrow$ https://keras.io/layers/recurrent/#gru\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "HRYnh8rfIBWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Bidirectional LSTMs (Exercise to run)\n",
        "\n",
        "Another well-known archicture is the bidirectional RNN. It is an extension of the regular RNN and usually offers a really good performance. Nowadays it is a standard archictecture in NLP.\n",
        "\n",
        "Note that RNNs (LSTMs and GRUs in a lesser degree) are dependent of the order and tent o remember the last words they have seen. A solution to this is to read the sentence from left-to-right and right-to-left, which can be implemented with bidirectional LSTMs.\n"
      ]
    },
    {
      "metadata": {
        "id": "LcpoiYARIBWW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "max_words = 10000\n",
        "embedding_size = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_size, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(lstm_hidden_size)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Br7zUSwPIBWa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_dev, y_dev), verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JPpcn1ZlIBWc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'dev'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXBC-QzPIBWe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Exercise 6\n",
        "You can try going further with, for example:\n",
        "- Try adjusting the number of units in the recurrent layer. \n",
        "- Try ```GRU```instead of ```LSTM```\n",
        "- Try to adjust the learning rate used in ```RMSprop```\n",
        "- Try some regularization techniques."
      ]
    }
  ]
}